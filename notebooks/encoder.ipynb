{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf5dfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec987ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"distilbert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18279b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc25b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 1005, 1049, 2747, 1999, 5522,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"I'm currently in Tokyo!\", return_tensors = \"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae5cf7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "    # query.size() = (batch_size, seq_size, head_dim)\n",
    "    dim_k = key.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "    # scores.size() = (batch_size, seq_size, seq_size)\n",
    "    weights = torch.nn.functional.softmax(scores, dim = -1)\n",
    "    # weights.size() = (batch_size, seq_size, seq_size)\n",
    "    return torch.bmm(weights, value) # (batch_size, seq_size, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1a98109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, embed_dim: int, head_dim: int):\n",
    "        super().__init__()\n",
    "        self.query = torch.nn.Linear(embed_dim, head_dim)\n",
    "        self.key = torch.nn.Linear(embed_dim, head_dim)\n",
    "        self.value = torch.nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.size() = (batch_size, seq_size, embed_dim)\n",
    "        x = scaled_dot_attention(\n",
    "            self.query(x),\n",
    "            self.key(x),\n",
    "            self.value(x)\n",
    "        )\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, config: AutoConfig):\n",
    "        super().__init__()\n",
    "        embed_dim = config.dim\n",
    "        head_dim = config.dim // config.n_heads\n",
    "        self.attention_heads = torch.nn.ModuleList([\n",
    "            AttentionHead(embed_dim, head_dim) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.output_layer = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x.size() = (batch_size, seq_size, embed_dim)\n",
    "        x = torch.concat([head(x) for head in self.attention_heads], dim = -1)\n",
    "        # x.size() = (batch_size, seq_size, embed_dim)\n",
    "        x = self.output_layer(x)\n",
    "        # x.size() = (batch_size, seq_size, embed_dim)\n",
    "        return x\n",
    "    \n",
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, config: AutoConfig):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings = config.vocab_size,\n",
    "            embedding_dim = config.dim,\n",
    "        )\n",
    "        self.position_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings = config.max_position_embeddings,\n",
    "            embedding_dim = config.dim,\n",
    "        )\n",
    "        self.layer_norm = torch.nn.LayerNorm(normalized_shape=config.dim)\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        # input_ids.size() = (batch_size, seq_size)\n",
    "        seq_size = input_ids.size(-1)\n",
    "        # this step creates (1, seq_size) tensor\n",
    "        positional_ids = torch.arange(seq_size, dtype = torch.long).unsqueeze(0)\n",
    "        token_embeddings = self.token_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(positional_ids)\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        # embeddings.size() = (batch_size, seq_size, embed_dim)\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "        \n",
    "class FNN(torch.nn.Module):\n",
    "    def __init__(self, embed_dim: int, inter_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear_layer_1 = torch.nn.Linear(embed_dim, inter_dim)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        self.linear_layer_2 = torch.nn.Linear(inter_dim, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear_layer_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_layer_2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class EncoderClassification(torch.nn.Module):\n",
    "    def __init__(self, config: AutoConfig):\n",
    "        super().__init__()\n",
    "        # embedding\n",
    "        self.embedding = Embedding(config)\n",
    "        # attention heads\n",
    "        self.attention_heads = MultiHeadAttention(config)\n",
    "        # FNN\n",
    "        self.fnn = FNN(config.dim, config.hidden_dim)\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(input_ids)\n",
    "        x = self.attention_heads(x)[:, 0, :]\n",
    "        x = self.fnn(x)[:, 0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f092cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b66ee610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5193], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0c83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
