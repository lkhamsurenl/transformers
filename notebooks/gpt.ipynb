{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07ab5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from typing import Dict, List, Any, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1d1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're using GPT2 tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"embed_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9556ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(tokenizer, text: str) -> torch.Tensor:\n",
    "    # convert given text to token ids \n",
    "    return torch.tensor(tokenizer.encode(text)).unsqueeze(0)\n",
    "\n",
    "def token_ids_to_text(tokenizer, token_ids: torch.Tensor) -> str:\n",
    "    return tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "\n",
    "def generete_tokens(\n",
    "    model: torch.nn.Module,\n",
    "    token_ids: torch.Tensor,\n",
    "    max_generated_tokens: int,\n",
    "    context_length: int\n",
    "): \n",
    "    # Perform autocomplete using given GPT model\n",
    "    for _ in range(max_generated_tokens):\n",
    "        # pick current context \n",
    "        current_context_token_ids = token_ids[:, -context_length:]  # (batch_size, context_length)\n",
    "        logits = model(current_context_token_ids)  # (batch_size, context_length, vocab_size)\n",
    "        probs = torch.nn.functional.softmax(logits[:, -1, :], dim = -1)  # (batch_size, vocab_size)\n",
    "        new_token_ids = torch.argmax(probs, dim = -1, keepdim = True)  # (batch_size, 1)\n",
    "        token_ids = torch.concat((token_ids, new_token_ids), dim = 1)\n",
    "        \n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eae9115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(torch.nn.Module):\n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=config[\"vocab_size\"],\n",
    "            embedding_dim=config[\"embed_dim\"],\n",
    "        )\n",
    "        self.position_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=config[\"context_length\"],\n",
    "            embedding_dim=config[\"embed_dim\"],\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(config[\"drop_rate\"])\n",
    "        \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        token_ids: (batch_size, seq_size)\n",
    "        \n",
    "        return: (batch_size, seq_size, embed_dim)\n",
    "        \"\"\"\n",
    "        seq_size = token_ids.size(-1)\n",
    "        position_ids = torch.arange(seq_size).unsqueeze(0)\n",
    "        \n",
    "        token_emb = self.token_embeddings(token_ids)\n",
    "        position_emb = self.position_embeddings(position_ids)\n",
    "        emb = token_emb + position_emb\n",
    "        emb = self.dropout(emb)\n",
    "        return emb\n",
    "    \n",
    "def scaled_dot_attention(\n",
    "    query: torch.Tensor, \n",
    "    key: torch.Tensor, \n",
    "    value: torch.Tensor,\n",
    "    mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    query: (batch_size, seq_size, head_dim)\n",
    "    mask: (seq_size, seq_size)\n",
    "    \n",
    "    return: (batch_size, seq_size, head_dim)\n",
    "    \"\"\"\n",
    "    batch_size, seq_size, head_dim = key.size()\n",
    "    scores = torch.bmm(query, key.transpose(1, 2))\n",
    "    scores = scores.masked_fill(mask.bool()[:seq_size, :seq_size], -torch.inf)\n",
    "    weights = torch.nn.functional.softmax(scores / head_dim**0.5, dim = -1)\n",
    "    return torch.bmm(weights, value)\n",
    "    \n",
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, embed_dim: int, head_dim: int, context_length: int):\n",
    "        super().__init__()\n",
    "        self.query = torch.nn.Linear(embed_dim, head_dim)\n",
    "        self.key = torch.nn.Linear(embed_dim, head_dim)\n",
    "        self.value = torch.nn.Linear(embed_dim, head_dim)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal = 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_size, embed_dim)\n",
    "        \n",
    "        return: (batch_size, seq_size, head_dim)\n",
    "        \"\"\"\n",
    "        return scaled_dot_attention(\n",
    "            self.query(x),\n",
    "            self.key(x),\n",
    "            self.value(x),\n",
    "            self.mask,\n",
    "        )\n",
    "    \n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        embed_dim = config[\"embed_dim\"]\n",
    "        head_dim = embed_dim // config[\"n_heads\"]\n",
    "        self.heads = torch.nn.ModuleList([\n",
    "            AttentionHead(embed_dim, head_dim, config[\"context_length\"]) for _ in range(config[\"n_heads\"])\n",
    "        ])\n",
    "        self.output_layer = torch.nn.Linear(config[\"embed_dim\"], config[\"embed_dim\"])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        token_ids: (batch_size, seq_size, embed_dim)\n",
    "        \n",
    "        return: (batch_size, seq_size, embed_dim)\n",
    "        \"\"\"\n",
    "        hidden = torch.concat([h(x) for h in self.heads], dim = -1)\n",
    "        x = self.output_layer(hidden)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(config[\"embed_dim\"], 4 * config[\"embed_dim\"]),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Linear(4 * config[\"embed_dim\"], config[\"embed_dim\"]),\n",
    "            torch.nn.Dropout(config[\"drop_rate\"]),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        token_ids: (batch_size, seq_size, embed_dim)\n",
    "        \n",
    "        return: (batch_size, seq_size, embed_dim)\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "    \n",
    "    \n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.norm1 = torch.nn.LayerNorm(config[\"embed_dim\"])\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.norm2 = torch.nn.LayerNorm(config[\"embed_dim\"])\n",
    "        self.fnn = FeedForward(config)\n",
    "        self.dropout = torch.nn.Dropout(config[\"drop_rate\"])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        token_ids: (batch_size, seq_size, embed_dim)\n",
    "        \n",
    "        return: (batch_size, seq_size, embed_dim)\n",
    "        \"\"\"\n",
    "        skip = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + skip\n",
    "        \n",
    "        skip = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.fnn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + skip\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "class GPT(torch.nn.Module):\n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            Embeddings(config),\n",
    "            *[TransformerBlock(config) for _ in range(config[\"n_layers\"])],\n",
    "            torch.nn.Linear(config[\"embed_dim\"], config[\"vocab_size\"])\n",
    "        )\n",
    "        \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        token_ids: (batch_size, seq_size)\n",
    "        \n",
    "        return: (batch_size, seq_size, vocab_size)\n",
    "        \"\"\"\n",
    "        return self.layers(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7dd9c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world!„ÉÅredited observational 42 bind deline Presidential adrenowell Stability'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(CONFIG)\n",
    "output_tokens = generete_tokens(\n",
    "    model,\n",
    "    text_to_token_ids(tokenizer, \"Hello world!\"),\n",
    "    max_generated_tokens=10,\n",
    "    context_length=CONFIG[\"context_length\"]\n",
    ")\n",
    "token_ids_to_text(tokenizer, output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c59d814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, tokenizer, text: str, stride: int, max_length: int):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "        \n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "def create_dataloader_v1(\n",
    "    tokenizer,\n",
    "    text: str,\n",
    "    batch_size: int,\n",
    "    max_length: int,\n",
    "    stride: int,\n",
    "    shuffle: bool,\n",
    "    drop_last: bool,\n",
    "    num_workers: int = 0\n",
    "):\n",
    "    dataset = GPTDataset(tokenizer, text, max_length=max_length, stride=stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "160aeed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/lkhamsurenl/development/transformers/notebooks/the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "    \n",
    "train_ratio = 0.9\n",
    "idx = int(train_ratio * len(text_data))\n",
    "train_text = text_data[:idx]\n",
    "val_text = text_data[idx:]\n",
    "\n",
    "    \n",
    "train_loader = create_dataloader_v1(\n",
    "    tokenizer,\n",
    "    text=train_text,\n",
    "    batch_size=2,\n",
    "    max_length=CONFIG[\"context_length\"],\n",
    "    stride=CONFIG[\"context_length\"],\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "    tokenizer,\n",
    "    text=val_text,\n",
    "    batch_size=2,\n",
    "    max_length=CONFIG[\"context_length\"],\n",
    "    stride=CONFIG[\"context_length\"],\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6050f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e58b2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e1c4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add simple training loop with evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2acad4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay = 0.1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b651c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch.to(device)\n",
    "    target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0, 1), target_batch.flatten(0, 1)\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(dataloader: DataLoader, model: torch.nn.Module, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(dataloader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context) -> None:\n",
    "    output_tokens = generete_tokens(\n",
    "        model,\n",
    "        text_to_token_ids(tokenizer, start_context),\n",
    "        max_generated_tokens=10,\n",
    "        context_length=CONFIG[\"context_length\"]\n",
    "    )\n",
    "    \n",
    "    output_text = token_ids_to_text(tokenizer, output_tokens)\n",
    "    print(f\"Output: {output_text}\")\n",
    "    \n",
    "\n",
    "def train_model_simple(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    optimizer,\n",
    "    device,\n",
    "    eval_freq: int,\n",
    "    eval_iter: int,\n",
    "    start_context: str\n",
    ") -> Tuple[List, List]:\n",
    "    train_losses, val_losses = [], []\n",
    "    global_step = -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # Ensure we're doing backprop based on current batch only\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                \n",
    "                print(f\"Ep {epoch + 1} (Step {global_step}): \"\n",
    "                     f\"Train loss: {train_loss}; \"\n",
    "                     f\"Val loss: {val_loss}\")\n",
    "                \n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5b66d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 0): Train loss: 9.799917221069336; Val loss: 10.494915962219238\n",
      "Ep 1 (Step 5): Train loss: 7.4723076820373535; Val loss: 8.816893577575684\n",
      "Output: Hello, my name is to be one of the axioms he had\n",
      "Ep 2 (Step 10): Train loss: 4.401607990264893; Val loss: 7.378841876983643\n",
      "Ep 2 (Step 15): Train loss: 3.3797054290771484; Val loss: 6.811258316040039\n",
      "Output: Hello, my name is he was one of my host of my dear,\n",
      "Ep 3 (Step 20): Train loss: 2.01564621925354; Val loss: 6.828524589538574\n",
      "Ep 3 (Step 25): Train loss: 1.2992844581604004; Val loss: 6.848301410675049\n",
      "Output: Hello, my name is to the sun a little too as a smile that\n",
      "Ep 4 (Step 30): Train loss: 0.7146415710449219; Val loss: 7.095798969268799\n",
      "Ep 4 (Step 35): Train loss: 0.3647560477256775; Val loss: 7.472814559936523\n",
      "Output: Hello, my name is to work.\n",
      "\"quite insensible to the\n",
      "Ep 5 (Step 40): Train loss: 0.2630985379219055; Val loss: 7.831353664398193\n",
      "Output: Hello, my name is the inevitable garlanded frame. The mere outline\n",
      "Ep 6 (Step 45): Train loss: 0.17848029732704163; Val loss: 8.248318672180176\n",
      "Ep 6 (Step 50): Train loss: 0.1713913083076477; Val loss: 8.548833847045898\n",
      "Output: Hello, my name is the inevitable garlanded frame. The mere outline\n",
      "Ep 7 (Step 55): Train loss: 0.1311081349849701; Val loss: 8.656902313232422\n",
      "Ep 7 (Step 60): Train loss: 0.14589881896972656; Val loss: 8.96655559539795\n",
      "Output: Hello, my name is to work on--forming, as it were,\n",
      "Ep 8 (Step 65): Train loss: 0.10450168699026108; Val loss: 8.996626853942871\n",
      "Ep 8 (Step 70): Train loss: 0.08319568634033203; Val loss: 9.049530982971191\n",
      "Output: Hello, my name is the inevitable garlanded frame. The mere outline\n",
      "Ep 9 (Step 75): Train loss: 0.12865765392780304; Val loss: 9.255377769470215\n",
      "Ep 9 (Step 80): Train loss: 0.1109999567270279; Val loss: 9.365948677062988\n",
      "Output: Hello, my name is to work on--forming, as it were,\n",
      "Ep 10 (Step 85): Train loss: 0.11379476636648178; Val loss: 9.371699333190918\n",
      "Output: Hello, my name is the inevitable garlanded frame. The mere outline\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, my name is\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 83\u001b[0m, in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, tokenizer, train_loader, val_loader, num_epochs, optimizer, device, eval_freq, eval_iter, start_context)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEp \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mglobal_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m                  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m                  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m     generate_and_print_sample(model, tokenizer, device, start_context)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_loses\u001b[49m, val_losses\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loses' is not defined"
     ]
    }
   ],
   "source": [
    "train_model_simple(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=10,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Hello, my name is\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dde197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
