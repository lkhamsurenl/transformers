{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b014b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f775875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_attention(\n",
    "    query: torch.Tensor, \n",
    "    key: torch.Tensor, \n",
    "    value: torch.Tensor, \n",
    "    masked: bool \n",
    ") -> torch.Tensor: \n",
    "    # query.size() = (batch_size, seq_size, head_dim)\n",
    "    # TODO: add masking operation\n",
    "    dim_k = key.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "    # scores.size() = (batch_size, seq_size, seq_size)\n",
    "    if masked:\n",
    "        mask = torch.triu(scores)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    weights = torch.nn.functional.softmax(scores, dim = -1)\n",
    "    output = torch.bmm(weights, value)\n",
    "    # output.size() = (batch_size, seq_size, head_dim)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "668cd4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"distilbert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bba1b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.46.3\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb1522f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hawaiian white Christmas\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83085f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(torch.nn.Module):\n",
    "    def __init__(self, config: AutoConfig):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(\n",
    "            num_embeddings = config.vocab_size,\n",
    "            embedding_dim = config.dim,\n",
    "        )\n",
    "        self.position_embedding = torch.nn.Embedding(\n",
    "            num_embeddings = config.max_position_embeddings,\n",
    "            embedding_dim = config.dim,\n",
    "        )\n",
    "        self.layer_norm = torch.nn.LayerNorm(\n",
    "            normalized_shape = config.dim,\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        token_ids: (batch_size, seq_size)\n",
    "        return: (batch_size, seq_size, embed_dim)\n",
    "        \"\"\"\n",
    "        seq_size = token_ids.size(-1)\n",
    "        position_ids = torch.arange(0, seq_size).unsqueeze(0)\n",
    "        token_embedding = self.token_embedding(token_ids)  # (batch_size, seq_size, embed_dim)\n",
    "        position_embedding = self.position_embedding(position_ids)  # (batch_size, seq_size, embed_dim)\n",
    "        embedding = token_embedding + position_embedding\n",
    "        embedding = self.layer_norm(embedding)\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding  \n",
    "    \n",
    "class AttentionHead(torch.nn.Module):\n",
    "    def __init__(self, embed_dim: int, head_dim: int):\n",
    "        super().__init__()\n",
    "        self.query = torch.nn.Linear(embed_dim, head_dim)\n",
    "        self.key = torch.nn.Linear(embed_dim, head_dim)\n",
    "        self.value = torch.nn.Linear(embed_dim, head_dim)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_size, embed_dim)\n",
    "        return: (batch_size, seq_size, head_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Add mask\n",
    "        return scaled_dot_attention(\n",
    "            self.query(x),\n",
    "            self.key(x),\n",
    "            self.value(x),\n",
    "            None\n",
    "        )\n",
    "\n",
    "class AttentionHeads(torch.nn.Module):\n",
    "    def __init__(self, config: AutoConfig):\n",
    "        super().__init__()\n",
    "        embed_dim = config.dim\n",
    "        head_dim = config.dim // config.n_heads\n",
    "        self.heads = torch.nn.ModuleList([\n",
    "            AttentionHead(embed_dim, head_dim) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.output_layer = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        hidden_state: (batch_size, seq_size, embed_dim)\n",
    "        return: (batch_size, seq_size, embed_dim)\n",
    "        \"\"\"\n",
    "        # \n",
    "        x = torch.concat([head(hidden_state) for head in self.heads], dim = -1)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class FeedForwardNet(torch.nn.Module):\n",
    "    def __init__(self, config: AutoConfig):\n",
    "        super().__init__()\n",
    "        self.linear_layer_1 = torch.nn.Linear(config.dim, config.hidden_dim)\n",
    "        self.gelu = torch.nn.GELU()\n",
    "        self.dropout = torch.nn.Dropout()\n",
    "        self.linear_layer_2 = torch.nn.Linear(config.hidden_dim, config.vocab_size)\n",
    "        self.softmax = torch.nn.Softmax(dim = -1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        hidden_state: (batch_size, embed_dim)\n",
    "        return: (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        x = self.linear_layer_1(x)  # (batch_size, hidden_dim)\n",
    "        x = self.gelu(x)  # (batch_size, hidden_dim)\n",
    "        x = self.dropout(x)  # (batch_size, hidden_dim)\n",
    "        x = self.linear_layer_2(x)  # (batch_size, vocab_size)\n",
    "        x = self.softmax(x)  # (batch_size, vocab_size)\n",
    "        return x\n",
    "        \n",
    "class GPT(torch.nn.Module):\n",
    "    def __init__(self, config: AutoConfig):\n",
    "        super().__init__()\n",
    "        # Embedding for tokens\n",
    "        self.embeddings = Embeddings(config)\n",
    "        # attention heads\n",
    "        self.attention_heads = AttentionHeads(config)\n",
    "        # FNN\n",
    "        self.feedforward = FeedForwardNet(config)\n",
    "        \n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        token_ids: (batch_size, seq_size)\n",
    "        return: (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        x = self.embeddings(token_ids)  # (batch_size, seq_size, embed_dim)\n",
    "        x = self.attention_heads(x)[:, 0, :]  # (batch_size, embed_dim)\n",
    "        x = self.feedforward(x)  # (batch_size, vocab_size)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfef541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af16162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.2961e-05, 3.2880e-05, 3.4805e-05,  ..., 3.3031e-05, 3.4138e-05,\n",
       "         3.3315e-05]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ebb770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
